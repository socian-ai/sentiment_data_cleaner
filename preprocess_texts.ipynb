{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from langdetect import detect as lang_detect\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from banglakit import lemmatizer as lem\n",
    "from banglakit.lemmatizer import BengaliLemmatizer\n",
    "\n",
    "lemmatizer = BengaliLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHAR_LEN=150\n",
    "MIN_CHAR_LEN=10\n",
    "MIN_WORD_LEN= 3\n",
    "SIMILARITY_THRESHOLD = .7\n",
    "\n",
    "input_text_path = '/hdd/sifat/NLP/sentiment_analysis/data/news-analyzer1.json'\n",
    "output_csv_path = 'news_1k_articles_15k.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the raw text source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[]\n",
    "\n",
    "with open(input_text_path) as input_f:\n",
    "    for obj in input_f:\n",
    "        text_list.append(obj)\n",
    "        \n",
    "print('Loaded {} lines from {} file'.format(len(text_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    garbage_list= \"'‘’“”/\\\"—|\"  ## Removes these characters\n",
    "    clean_text= \"\"\n",
    "    for char in text:\n",
    "        if char not in garbage_list:\n",
    "            clean_text+=char\n",
    "    return clean_text\n",
    "\n",
    "def is_all_bangla(text):\n",
    "    if bool(re.match(\"^[\\u0980-\\u09FF ।,?!.]+$\",text)):  ## only valid characters\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_valid(text):\n",
    "    l= len(text)\n",
    "    if l<MIN_CHAR_LEN or l>MAX_CHAR_LEN:\n",
    "        return False\n",
    "    if len(text.split())<MIN_WORD_LEN:\n",
    "        return False\n",
    "    if not is_all_bangla(text):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def lemmatize_sentence(text):\n",
    "    result=\"\"\n",
    "    for word in text.split():\n",
    "        lem_word= lemmatizer.lemmatize(word, pos=lem.POS_NOUN)\n",
    "        if is_all_bangla(lem_word):\n",
    "            result+= lem_word + \" \"\n",
    "        else:\n",
    "            result+= word + \" \"\n",
    "    return result\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences= []\n",
    "lem_sentences= []\n",
    "\n",
    "for text in text_list:\n",
    "    article,article_url= json.loads(text)[\"_source\"][\"body\"], json.loads(text)[\"_source\"][\"url\"]\n",
    "    for m in article.split(\"।\"):\n",
    "        clean_m= clean(m)\n",
    "        if is_valid(clean_m.strip()):\n",
    "            clean_sentences.append((clean_m.strip(),article_url))\n",
    "            lem_sentences.append(lemmatize_sentence(clean_m.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total articles {total} Valid and clean sentences {len(clean_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Checker based on jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(\"নতুন সিমে কয় টাকা লোড দিলে এক জিবি নেট পাওয়া যায়\".split(),\"নতুন সিমে কতো টাকা লোড দিলে এক জিবি নেট পাওয়া যাবে\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_sentences= []\n",
    "for clean_idx,(sentence,s_id) in tqdm(enumerate(clean_sentences)):\n",
    "    is_unique=True\n",
    "    l1= lemmatize_sentence(sentence)\n",
    "    for lem_idx,l2 in enumerate(lem_sentences):\n",
    "        if clean_idx==lem_idx:\n",
    "            continue\n",
    "        if jaccard_similarity(l1.split(),l2.split()) > SIMILARITY_THRESHOLD:\n",
    "            is_unique=False\n",
    "            break\n",
    "    \n",
    "    if is_unique:\n",
    "        final_sentences.append((sentence,s_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total {total} unique based on jaccard similarity {len(final_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(pd.DataFrame(final_sentences),\"news_1k_articles_15k.csv\",header=None,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
